from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

# ëª¨ë¸ ID ë° í† í° ë¶ˆëŸ¬ì˜¤ê¸°
model_id = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
hf_token = os.getenv("HF_TOKEN")

# í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    token=hf_token
)

# í…ŒìŠ¤íŠ¸ ì…ë ¥ (LLaMA 3 ì±„íŒ… ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸)
user_input = "ì†í¥ë¯¼ì€ ëª‡ì‚´ì´ì•¼?"
prompt = f"<|begin_of_text|><|user|>\n{user_input}<|end_of_text|>\n<|assistant|>\n"

# í† í°í™” ë° ëª¨ë¸ ì¶”ë¡ 
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        temperature=0.7,
        do_sample=True,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id
    )

# ì¶œë ¥ ë””ì½”ë”©
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("ğŸ§  ëª¨ë¸ ì‘ë‹µ:")
print(response.replace(prompt, "").strip())