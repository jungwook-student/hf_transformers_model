from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

print("ğŸ”§ ì‹œì‘: í™˜ê²½ ë³€ìˆ˜ í™•ì¸")
model_id = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
hf_token = os.getenv("HF_TOKEN")

if hf_token:
    print("âœ… HF_TOKEN í™˜ê²½ ë³€ìˆ˜ í™•ì¸ë¨")
else:
    print("âŒ HF_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ â€” ì¢…ë£Œí•©ë‹ˆë‹¤.")
    exit(1)

print("ğŸ“¦ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘...")
tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)
print("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

print("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì‹œì‘ (GPU ê°•ì œ í• ë‹¹)...")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map={"": 0},  # GPU 0ë²ˆ ê°•ì œ ì§€ì •
    token=hf_token
)
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

print("ğŸ“ í”„ë¡¬í”„íŠ¸ êµ¬ì„±...")
user_input = "ì†í¥ë¯¼ì€ ëª‡ì‚´ì´ì•¼?"
prompt = f"<|begin_of_text|><|user|>\n{user_input}<|end_of_text|>\n<|assistant|>\n"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
print("âœ… ì…ë ¥ í† í° ë³€í™˜ ì™„ë£Œ")

print("ğŸ§  ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        temperature=0.7,
        do_sample=True,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id
    )
print("âœ… ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ")

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("ğŸ§¾ ëª¨ë¸ ì‘ë‹µ:")
print(response.replace(prompt, "").strip())