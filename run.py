from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
import subprocess
import time

print("ğŸ”§ ì‹œì‘: í™˜ê²½ ë³€ìˆ˜ í™•ì¸")
model_id = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
hf_token = os.getenv("HF_TOKEN")

if hf_token:
    print("âœ… HF_TOKEN í™˜ê²½ ë³€ìˆ˜ í™•ì¸ë¨")
else:
    print("âŒ HF_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ â€” ì¢…ë£Œí•©ë‹ˆë‹¤.")
    exit(1)

print("ğŸ“¦ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘...")
tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)
print("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

# ìºì‹œ ê²½ë¡œ ì„¤ì •
cache_dir = os.path.expanduser("~/.cache/huggingface")

print("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì‹œì‘ (device_map='auto')...")
print(f"ğŸ“ í˜„ì¬ ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")

# ìºì‹œ ë³€í™” ì¶”ì 
def show_cache_size():
    try:
        output = subprocess.check_output(["du", "-sh", cache_dir])
        print(f"ğŸ“¦ ìºì‹œ í¬ê¸°: {output.decode('utf-8').strip()}")
    except Exception as e:
        print(f"âš ï¸ ìºì‹œ í™•ì¸ ì‹¤íŒ¨: {e}")

# ìºì‹œ ìƒíƒœ ì£¼ê¸°ì ìœ¼ë¡œ ì¶œë ¥ (ë°°ê²½ì—ì„œ)
for i in range(3):
    show_cache_size()
    time.sleep(5)

# ëª¨ë¸ ë¡œë”© (ì´ ë‹¨ê³„ì—ì„œ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    token=hf_token
)
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì¶”ë¡ 
user_input = "ì†í¥ë¯¼ì€ ëª‡ì‚´ì´ì•¼?"
prompt = f"<|begin_of_text|><|user|>\n{user_input}<|end_of_text|>\n<|assistant|>\n"

print("ğŸ“ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ")
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

print("ğŸ§  ì¶”ë¡  ì‹œì‘...")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        temperature=0.7,
        do_sample=True,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id
    )
print("âœ… ì¶”ë¡  ì™„ë£Œ")

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("ğŸ§¾ ëª¨ë¸ ì‘ë‹µ:")
print(response.replace(prompt, "").strip())