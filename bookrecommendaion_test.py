import json
import torch
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import requests
import difflib

# 1. íŠœë‹ëœ ëª¨ë¸ ë¡œë”©
def load_model():
    base = AutoModelForCausalLM.from_pretrained(
        "davidkim205/komt-mistral-7b-v1",
        device_map="auto",
        torch_dtype=torch.float16,
        quantization_config=BitsAndBytesConfig(load_in_4bit=True),
        use_safetensors=True
    )
    model = PeftModel.from_pretrained(base, "JungWook0116/ko-mistral7b-v1-book-recommender")
    tokenizer = AutoTokenizer.from_pretrained("davidkim205/komt-mistral-7b-v1")
    return model.eval(), tokenizer

# 2. ì¡°ê±´ ì¶”ì¶œ í•¨ìˆ˜
def extract_conditions(model, tokenizer, sentence: str):
    prompt = f"""### Instruction:
ë‹¤ìŒ ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ ë„ì„œ ì¶”ì²œ ì¡°ê±´ì„ ì¶”ì¶œí•˜ì„¸ìš”.

### Input:
{sentence}

### Output:"""
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
    with torch.no_grad():
        outputs = model.generate(input_ids, max_new_tokens=50, do_sample=False)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    parsed = decoded.split("### Output:")[-1].strip()
    return parse_extracted(parsed)

# 3. ì¶”ì¶œëœ ë¬¸ìì—´ íŒŒì‹±
def parse_extracted(text):
    result = {"theme": [], "type": None, "age": None}
    for part in text.split(","):
        part = part.strip()
        if part.startswith("theme="):
            result["theme"] = [t.strip() for t in part[len("theme="):].split()]  # ê³µë°± ê¸°ì¤€ split
        elif part.startswith("type="):
            result["type"] = part[len("type="):].strip()
        elif part.startswith("age="):
            result["age"] = part[len("age="):].strip()
    return result

# 4. ë„ì„œ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ (í™•ì¥ëœ ìœ ì‚¬ë„ ê¸°ì¤€)
def similarity_score(a, b):
    return difflib.SequenceMatcher(None, a, b).ratio()

def age_match_score(user_age_str, book_age_str):
    import re
    try:
        user_age = int(re.findall(r'\d+', user_age_str)[0])
        book_ages = [int(x) for x in re.findall(r'\d+', book_age_str)]
        return 1.0 if user_age in book_ages else 0.0
    except:
        return 0.0

def score_books(books, extracted):
    results = []
    for book in books:
        if "theme" not in book or "types" not in book or "age" not in book:
            continue
        theme_score = max([similarity_score(t, bt) for t in extracted["theme"] for bt in book["theme"]]) if extracted["theme"] else 0.0
        type_score = max([similarity_score(extracted["type"], bt) for bt in book["types"]]) if extracted["type"] else 0.0
        age_score = age_match_score(extracted["age"], book["age"]) if extracted["age"] else 0.0
        final_score = (theme_score + type_score + age_score) / 3
        results.append((final_score, book))
    results.sort(reverse=True, key=lambda x: x[0])
    return [b for s, b in results if s > 0]

# 5. FAISS ìœ ì‚¬ë„ ê²€ìƒ‰
def build_faiss_index(texts, model):
    vectors = model.encode(texts, convert_to_numpy=True)
    index = faiss.IndexFlatL2(vectors.shape[1])
    index.add(vectors)
    return index, vectors

def recommend_books(input_sentence, books, sbert_model, model, tokenizer, top_k=5):
    print("ğŸ§  ì¡°ê±´ ì¶”ì¶œ ì¤‘...")
    extracted = extract_conditions(model, tokenizer, input_sentence)
    print(f"ğŸ¯ ì¶”ì¶œëœ ì¡°ê±´: {extracted}")
    
    total_books = len(books)
    print(f"ğŸ“˜ ì „ì²´ ë„ì„œ ìˆ˜: {total_books}")
    candidates = score_books(books, extracted)
    print(f"âœ… ìŠ¤ì½”ì–´ë§ëœ ë„ì„œ ìˆ˜(ì ìˆ˜ > 0): {len(candidates)}")

    if not candidates:
        print("âŒ ì¶”ì²œí•  ë„ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    candidate_texts = [
        f"query: theme={' '.join(b['theme'])}, type={' '.join(b['types'])}, age={b['age']}" for b in candidates
    ]
    index, _ = build_faiss_index(candidate_texts, sbert_model)

    query = f"query: theme={' '.join(extracted['theme'])}, type={extracted['type']}, age={extracted['age']}"
    query_vec = sbert_model.encode([query])
    _, idxs = index.search(query_vec, top_k)
    return [candidates[i] for i in idxs[0]]

if __name__ == "__main__":
    print("ğŸ“¦ ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ì¤‘...")
    url = "https://raw.githubusercontent.com/jungwook-student/hf_transformers_model/main/aladin_fully_enriched_upto_now_552.json"
    response = requests.get(url)
    books = json.loads(response.text)
    missing_theme_count = sum(1 for b in books if "theme" not in b)
    print(f"ğŸš¨ 'theme' í•„ë“œê°€ ì—†ëŠ” ë„ì„œ ìˆ˜: {missing_theme_count}")

    sbert_model = SentenceTransformer("intfloat/multilingual-e5-base")
    model, tokenizer = load_model()

    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    user_input = "3ì„¸ ë‚¨ìì•„ì´ì—ê²Œ ì½ì–´ì¤„ ì°½ì˜ë ¥ì„ í‚¤ì›Œì¤„ ê·¸ë¦¼ì±…ì„ ì¶”ì²œí•´ì¤˜"
    results = recommend_books(user_input, books, sbert_model, model, tokenizer, top_k=5)

    print("\nğŸ” ì¶”ì²œ ë„ì„œ ê²°ê³¼:")
    for b in results:
        print(f"- {b['title']} ({b['age']}, {b['types']})")
