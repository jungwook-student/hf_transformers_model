import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
import torch
import json
import requests

# âœ… ëª¨ë¸ ë¡œë”©
@st.cache_resource
def load_models():
    adapter_model_id = "JungWook0116/ko-mistral7b-v1-book-recommender"
    base = AutoModelForCausalLM.from_pretrained(
        adapter_model_id, torch_dtype=torch.float16, device_map="auto", use_safetensors=True
    )
    tokenizer = AutoTokenizer.from_pretrained(adapter_model_id)
    sbert = SentenceTransformer("intfloat/multilingual-e5-base")
    return base, tokenizer, sbert

# âœ… ë„ì„œ ë°ì´í„° ë¡œë”©
@st.cache_data
def load_books():
    url = "https://raw.githubusercontent.com/jungwook-student/hf_transformers_model/main/aladin_fully_enriched_upto_now_552.json"
    data = requests.get(url).json()
    return [book for book in data if "theme" in book and "type" in book and "age" in book]

# âœ… ì¡°ê±´ ì¶”ì¶œ í•¨ìˆ˜
def extract_conditions(prompt, model, tokenizer):
    input_text = f"### Instruction:\në‹¤ìŒ ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ ë„ì„œ ì¶”ì²œ ì¡°ê±´ì„ ì¶”ì¶œí•˜ì„¸ìš”.\n\n### Input:\n{prompt}\n\n### Output:\n"
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(model.device)
    with torch.no_grad():
        output = model.generate(input_ids, max_new_tokens=50)
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    extracted = decoded.split("### Output:")[-1].strip()
    return extracted

# âœ… ìœ ì‚¬ë„ ê¸°ë°˜ ì¶”ì²œ
def recommend_books(prompt, model, tokenizer, sbert, books):
    extracted = extract_conditions(prompt, model, tokenizer)
    st.markdown(f"ğŸ¯ **ì¶”ì¶œëœ ì¡°ê±´**: `{extracted}`")

    query_emb = sbert.encode(extracted, convert_to_tensor=True)
    texts = [
        f"theme={' '.join(book['theme'])}, type={' '.join(book['type'])}, age={book['age']}"
        for book in books
    ]
    corpus_embs = sbert.encode(texts, convert_to_tensor=True)
    scores = util.cos_sim(query_emb, corpus_embs)[0]
    top_k = torch.topk(scores, k=5)

    st.markdown("ğŸ” **ì¶”ì²œ ë„ì„œ ê²°ê³¼:**")
    for idx in top_k.indices.tolist():
        b = books[idx]
        st.write(f"- **{b['title']}** ({b['age']}, {b['type']})")

# âœ… Streamlit UI êµ¬ì„±
st.title("ğŸ“š ìœ ì•„ ë„ì„œ ì¶”ì²œê¸°")

with st.spinner("ì´ˆê¸° ë¡œë”© ì¤‘ì…ë‹ˆë‹¤. ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ìˆìŠµë‹ˆë‹¤..."):
    model, tokenizer, sbert = load_models()
    books = load_books()

st.success("âœ… ì´ˆê¸°í™” ì™„ë£Œ! ì´ì œ ë¬¸ì¥ì„ ì…ë ¥í•´ë³´ì„¸ìš”.")
user_input = st.text_input("ì¶”ì²œì„ ìœ„í•œ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:")

if user_input:
    with st.spinner("ì¶”ì²œ ì¤‘..."):
        recommend_books(user_input, model, tokenizer, sbert, books)
