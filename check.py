from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# ëª¨ë¸ ë° adapter ê²½ë¡œ ì„¤ì •
base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"
adapter_path = "./outputs"  # í•™ìŠµëœ LoRA adapter ê²½ë¡œ

# ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)
model = PeftModel.from_pretrained(base_model, adapter_path)

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
test_inputs = [
    "ì²« ë“±ì›ì— ì½ê¸° ì¢‹ì€ ë†€ì´ì±…ì„ ì°¾ê³  ìˆì–´ìš”.",
    "ë™ë¬¼ì„ ë°°ìš¸ ìˆ˜ ìˆëŠ” ì±… ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”.",
    "ë†€ì´ì±… ì¤‘ì—ì„œ 4-6ì„¸ì„¸ê°€ ì¢‹ì•„í•  ë§Œí•œ ì±… ìˆì„ê¹Œìš”?",
    "2-4ì„¸ì„¸ ì•„ì´ë‘ ì½ê¸° ì¢‹ì€ ìì—° ì±… ì¶”ì²œí•´ ì£¼ì„¸ìš”.",
    "ìƒì¼ì— ì½ê¸° ì¢‹ì€ ë†€ì´ì±…ì„ ì°¾ê³  ìˆì–´ìš”."
]

# ì¶”ë¡  ë° ì¶œë ¥
for i, user_input in enumerate(test_inputs, 1):
    print(f"ğŸ‘¤ ì…ë ¥ {i}: {user_input}")
    prompt = f"### Instruction:\në‹¤ìŒ ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ ë„ì„œ ì¶”ì²œ ì¡°ê±´ì„ ì¶”ì¶œí•˜ì„¸ìš”.\n\n### Input:\n{user_input}\n\n### Response:\n"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = response.replace(prompt, "").strip()
    print(f"ğŸ¤– ì¶œë ¥ {i}:
{answer}
{'-'*50}")
